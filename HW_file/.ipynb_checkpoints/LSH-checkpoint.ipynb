{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import glob2\n",
    "import re, string\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"LSH\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = glob2.glob(\"athletics\\\\athletics\\\\*.txt\")\n",
    "article_list = article_list[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the path of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingle(article_dir):\n",
    "    art = sc.textFile(article_dir).collect()\n",
    "    name = article_dir.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    #print(\"file : \", name)\n",
    "    shingle = []\n",
    "    for i in range(0,len(art),2):\n",
    "        #print(\"ith paragraph : \",i)\n",
    "        out = re.sub('[%s]' % re.escape(string.punctuation), '', art[i])\n",
    "        word = out.split()\n",
    "        for j in range(len(word)):\n",
    "            key = tuple(word[j:j+3])\n",
    "            shingle.append((key,[name]))\n",
    "            if (len(word)-j) == 3:\n",
    "                break\n",
    "    return shingle\n",
    "\n",
    "Shingles = []\n",
    "for file in article_list:\n",
    "    shin = create_shingle(file)\n",
    "    Shingles = Shingles + shin\n",
    "random.shuffle(Shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the article and remove the punctuations and then split it into (3 shingles, file_name) and random suffle all shingles\n",
    "to prevent some potential bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_shingles = sc.parallelize(Shingles)\n",
    "All_shingles = list(All_shingles.reduceByKey(lambda p,q:p+q).collectAsMap().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge replicate singles and transform it into a list of to present the charateristic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Charateristic_Matrix = []\n",
    "for i in range(len(All_shingles)):\n",
    "    for j in All_shingles[i]:\n",
    "        x = (j,[i])\n",
    "        Charateristic_Matrix.append(x)\n",
    "Charateristic_Matrix = sc.parallelize(Charateristic_Matrix)\n",
    "Charateristic_Matrix = Charateristic_Matrix.reduceByKey(lambda p,q:p+q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform the characteristic matrix into (file_names, col_index) \n",
    "where the column index represents the index where the single locates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinHashing(random_vector_ab, prime_num, num_shingles):\n",
    "    \n",
    "    def min_hash(x):\n",
    "        key = x[0]\n",
    "        Signature = []\n",
    "        for j in random_vector_ab:\n",
    "            min_hash = []\n",
    "            for i in x[1]:\n",
    "                v = ((i*j[0]+j[1])%prime_num)%num_shingles\n",
    "                min_hash.append(v)\n",
    "            Signature.append(min(min_hash))\n",
    "        return (key,Signature)\n",
    "    return min_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a min hash function that runs through all hash functions and col_index and take the minimum one and form a list\n",
    "to present as signature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_vec_ab = np.random.randint(low=0, high=12983, size=(100,2))\n",
    "prime_number = 12983\n",
    "num_shingles = len(All_shingles)\n",
    "Signature_Matrix = Charateristic_Matrix.map(MinHashing(random_vec_ab,prime_number, num_shingles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the signature matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = Signature_Matrix.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_hashing(x):\n",
    "    num_bucket = 5000\n",
    "    prime_num = 5851\n",
    "    ab = np.random.randint(low=0, high=prime_num, size=(3,))\n",
    "    \n",
    "    for i in range(0,len(x[1]),2):\n",
    "        v = ((np.dot(x[1][i:i+2],ab[0:2])+ab[-1])%prime_num)%num_bucket\n",
    "        yield (v, [int(x[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function that hash the column of each file in 50 bands and 2 rows into buckets and return (bucket_num, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buckets = Signature_Matrix.flatMap(bucket_hashing).reduceByKey(lambda p,q:p+q).sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after getting the buckets and the file_names inside of it we merge ones with same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candidates = []\n",
    "for i in Buckets:\n",
    "    if len(i[1]) > 1:\n",
    "        Candidates.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out the candidate pairs that have been hash to the same bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard_similarity(list_1, list_2):\n",
    "    intersection = len(list(set(list_1).intersection(list_2)))\n",
    "    union = len(list_1) + len(list_2) - intersection\n",
    "    return float(intersection/union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define a function thay calculate the Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simiarities(candidate, Signature):\n",
    "    All_pairs = {}\n",
    "    for cand in candidate:\n",
    "        for i in range(len(cand[1])):\n",
    "            for j in range(i, len(cand[1])):\n",
    "                if i == j or cand[1][i] == cand[1][j]:\n",
    "                    continue\n",
    "                sim = Jaccard_similarity(Signature[str(cand[1][i]).zfill(3)],Signature[str(cand[1][j]).zfill(3)])\n",
    "                All_pairs[(str(cand[1][i]).zfill(3),str(cand[1][j]).zfill(3))] = sim\n",
    "    return All_pairs\n",
    "All_candidate_sim = Simiarities(Candidates,S)\n",
    "Sorted_Sim = {k:v for k,v in sorted(All_candidate_sim.items(), key=lambda item:item[1])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out the Jaccard similarity of each candidate pairs and sort by increasing orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 similar articles\n",
      "(('036', '018'), 0.22699386503067484)\n",
      "(('029', '002'), 0.2578616352201258)\n",
      "(('036', '026'), 0.25)\n",
      "(('024', '031'), 0.2422360248447205)\n",
      "(('018', '031'), 0.2422360248447205)\n",
      "(('036', '050'), 0.2422360248447205)\n",
      "(('022', '036'), 0.2422360248447205)\n",
      "(('036', '011'), 0.2345679012345679)\n",
      "(('039', '002'), 0.2345679012345679)\n",
      "(('022', '029'), 0.2345679012345679)\n"
     ]
    }
   ],
   "source": [
    "TOP_10 = list(Sorted_Sim.items())[-10:]\n",
    "print(\"TOP 10 similar articles\")\n",
    "for i in range(len(TOP_10)):\n",
    "    print(TOP_10[-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
